# build a cnn model

# cnn
# from torchinfo import summary
#
# torch.manual_seed(42)
# torch.cuda.manual_seed(42)
#
# model = torch.nn.Sequential()
#
# max_pool_size=4
# conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1)
# max_pool1 = torch.nn.MaxPool2d(max_pool_size)
# model.append(conv1)
# model.append(torch.nn.ReLU())
# model.append(max_pool1)
#
# conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)
# max_pool2 = torch.nn.MaxPool2d(max_pool_size)
# model.append(conv2)
# model.append(torch.nn.ReLU())
# model.append(max_pool2)
#
# conv3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
# max_pool3 = torch.nn.MaxPool2d(max_pool_size)
# model.append(conv3)
# model.append(torch.nn.ReLU())
# model.append(max_pool3)
#
# model.append(torch.nn.Flatten())
# model.append(torch.nn.Dropout())
#
# linear1 = torch.nn.Linear(in_features=576, out_features=500)
# model.append(linear1)
# model.append(torch.nn.ReLU())
# model.append(torch.nn.Dropout())
#
# n_classes = 2
# output_layer = torch.nn.Linear(in_features=500, out_features=n_classes)
# model.append(output_layer)
#
# summary(model, input_size=(batch_size, 3, 224, 224))

    # output cnn
    # ==========================================================================================
    # Layer (type:depth-idx)                   Output Shape              Param #
    # ==========================================================================================
    # Sequential                               [32, 2]                   --
    # ├─Conv2d: 1-1                            [32, 16, 224, 224]        448
    # ├─ReLU: 1-2                              [32, 16, 224, 224]        --
    # ├─MaxPool2d: 1-3                         [32, 16, 56, 56]          --
    # ├─Conv2d: 1-4                            [32, 32, 56, 56]          4,640
    # ├─ReLU: 1-5                              [32, 32, 56, 56]          --
    # ├─MaxPool2d: 1-6                         [32, 32, 14, 14]          --
    # ├─Conv2d: 1-7                            [32, 64, 14, 14]          18,496
    # ├─ReLU: 1-8                              [32, 64, 14, 14]          --
    # ├─MaxPool2d: 1-9                         [32, 64, 3, 3]            --
    # ├─Flatten: 1-10                          [32, 576]                 --
    # ├─Dropout: 1-11                          [32, 576]                 --
    # ├─Linear: 1-12                           [32, 500]                 288,500
    # ├─ReLU: 1-13                             [32, 500]                 --
    # ├─Dropout: 1-14                          [32, 500]                 --
    # ├─Linear: 1-15                           [32, 2]                   1,002
    # ==========================================================================================
    # Total params: 313,086
    # Trainable params: 313,086
    # Non-trainable params: 0
    # Total mult-adds (Units.GIGABYTES): 1.31
    # ==========================================================================================
    # Input size (MB): 19.27
    # Forward/backward pass size (MB): 234.55
    # Params size (MB): 1.25
    # Estimated Total Size (MB): 255.07
    # ==========================================================================================


# loss
# loss_fn = torch.nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#
# model.to(device)
#
# print(loss_fn)
# print('-'*40)
# print(optimizer)
# print('-'*40)
# print(next(model.parameters()).device)

    # output loss
    # CrossEntropyLoss()
    # ----------------------------------------
    # Adam (
    # Parameter Group 0
    #     amsgrad: False
    #     betas: (0.9, 0.999)
    #     capturable: False
    #     decoupled_weight_decay: False
    #     differentiable: False
    #     eps: 1e-08
    #     foreach: None
    #     fused: None
    #     lr: 0.001
    #     maximize: False
    #     weight_decay: 0
    # )
    # ----------------------------------------
    # mps:0

# train and evaluate model
# def train(model, dataloader, loss_fn, optimizer, device):
#     model.train()
#     total_loss, correct, total = 0, 0, 0
#
#     for images, labels in tqdm(dataloader, desc="Training", leave=False):
#         images, labels = images.to(device), labels.to(device)
#
#         # Forward pass
#         outputs = model(images)
#         loss = loss_fn(outputs, labels)
#
#         # Backward pass
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # Track metrics
#         total_loss += loss.item()
#         predicted = torch.argmax(outputs, dim=1)
#         total += labels.size(0)
#         correct += (predicted == labels).sum().item()
#
#     avg_loss = total_loss / len(dataloader)
#     accuracy = 100 * correct / total
#     return avg_loss, accuracy
#
#
# def evaluate(model, dataloader, loss_fn, device):
#     model.eval()
#     total_loss, correct, total = 0, 0, 0
#
#     with torch.no_grad():
#         for images, labels in tqdm(dataloader, desc="Evaluating", leave=False):
#             images, labels = images.to(device), labels.to(device)
#
#             outputs = model(images)
#             loss = loss_fn(outputs, labels)
#
#             total_loss += loss.item()
#             predicted = torch.argmax(outputs, dim=1)
#             total += labels.size(0)
#             correct += (predicted == labels).sum().item()
#
#     avg_loss = total_loss / len(dataloader)
#     accuracy = 100 * correct / total
#     return avg_loss, accuracy
#
#
# # combine them into one training loop
# train_losses, val_losses = [], []
# train_accuracies, val_accuracies = [], []
#
# epochs = 10
# for epoch in range(epochs):
#     train_loss, train_acc = train(model, train_dataloader, loss_fn, optimizer, device)
#     val_loss, val_acc = evaluate(model, val_dataloader, loss_fn, device)
#
#     train_losses.append(train_loss)
#     val_losses.append(val_loss)
#     train_accuracies.append(train_acc)
#     val_accuracies.append(val_acc)
#
#
#     print(f"Epoch [{epoch+1}/{epochs}]")
#     print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
#     print(f"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%")
#     print("-" * 50)

    # output train and eval
    # Epoch [1/10]
    #   Train Loss: 0.7024 | Train Acc: 51.47%
    #   Val Loss:   0.6888 | Val Acc:   50.00%
    # --------------------------------------------------
    #
    # Epoch [2/10]
    #   Train Loss: 0.6774 | Train Acc: 54.41%
    #   Val Loss:   0.6577 | Val Acc:   62.50%
    # --------------------------------------------------
    #
    # Epoch [3/10]
    #   Train Loss: 0.6662 | Train Acc: 54.41%
    #   Val Loss:   0.6583 | Val Acc:   68.75%
    # --------------------------------------------------
    #
    # Epoch [4/10]
    #   Train Loss: 0.6775 | Train Acc: 57.35%
    #   Val Loss:   0.6675 | Val Acc:   56.25%
    # --------------------------------------------------
    #
    # Epoch [5/10]
    #   Train Loss: 0.6625 | Train Acc: 60.29%
    #   Val Loss:   0.7016 | Val Acc:   56.25%
    # --------------------------------------------------
    #
    # Epoch [6/10]
    #   Train Loss: 0.6192 | Train Acc: 70.59%
    #   Val Loss:   0.6891 | Val Acc:   56.25%
    # --------------------------------------------------
    #
    # Epoch [7/10]
    #   Train Loss: 0.6954 | Train Acc: 66.18%
    #   Val Loss:   0.6259 | Val Acc:   75.00%
    # --------------------------------------------------
    #
    # Epoch [8/10]
    #   Train Loss: 0.6452 | Train Acc: 66.18%
    #   Val Loss:   0.5949 | Val Acc:   75.00%
    # --------------------------------------------------
    #
    # Epoch [9/10]
    #   Train Loss: 0.5538 | Train Acc: 75.00%
    #   Val Loss:   0.6243 | Val Acc:   62.50%
    # --------------------------------------------------
    #
    # Epoch [10/10]
    #   Train Loss: 0.6666 | Train Acc: 58.82%
    #   Val Loss:   0.5305 | Val Acc:   87.50%
    # --------------------------------------------------

# plot loss and accuracy over epochs
# plt.figure(figsize=(12, 5))
#
# plt.subplot(1, 2, 1)
# plt.plot(train_losses, label='Train Loss')
# plt.plot(val_losses, label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.legend()
#
# plt.subplot(1, 2, 2)
# plt.plot(train_accuracies, label='Train Accuracy')
# plt.plot(val_accuracies, label='Validation Accuracy')
# plt.title('Training and Validation Accuracy')
# plt.legend()
# plt.show()